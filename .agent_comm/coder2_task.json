{
  "agent_id": "coder2",
  "task_id": "task_2",
  "files": [
    {
      "filename": "simulation_environment.py",
      "purpose": "Implementation of the simulation environment",
      "priority": "medium",
      "dependencies": [
        "Python",
        "Matplotlib"
      ],
      "key_functions": [
        "initialize_simulation",
        "run_simulation"
      ],
      "estimated_lines": 200,
      "complexity": "low"
    }
  ],
  "project_info": {
    "project_name": "Human_Reasons_Based_Supervision_Framework",
    "project_type": "automated_vehicles",
    "description": "A framework for ethical decision-making in automated vehicles through human reasons-based supervision, enabling vehicles to evaluate and adjust their behavior according to human priorities and ethical considerations.",
    "key_algorithms": [
      "Human_Reasons-Based_Supervision",
      "Motion_Planning",
      "Model_Predictive_Control",
      "A* Search",
      "Bicycle_Model"
    ],
    "main_libraries": [
      "Python",
      "NumPy",
      "SciPy",
      "Matplotlib",
      "Scikit-learn"
    ]
  },
  "paper_content": "PDF: cs.SY_2507.23308v1_A-Framework-for-Ethical-Decision-Making-in-Automat.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nA Framework for Ethical Decision-Making in Automated Vehicles\nthrough Human Reasons-based Supervision\nLucas Elbert Suryana1,3, Saeed Rahmani1, Simeon C. Calvert1,3, Arkady Zgonnikov2,3, Bart van Arem1\nAbstract \u2014 Ethical dilemmas are a common challenge in\neveryday driving, requiring human drivers to balance com-\npeting priorities such as safety, efficiency, and rule compliance.\nHowever, much of the existing research in automated vehicles\n(A Vs) has focused on high-stakes \u201dtrolley problems,\u201d which\ninvolve extreme and rare situations. Such scenarios, though\nrich in ethical implications, are rarely applicable in real-\nworld A V decision-making. In practice, when A Vs confront\neveryday ethical dilemmas, they often appear to prioritise strict\nadherence to traffic rules. By contrast, human drivers may\nbend the rules in context-specific situations, using judgement\ninformed by practical concerns such as safety and efficiency.\nAccording to the concept of meaningful human control, A Vs\nshould respond to human reasons, including those of drivers,\nvulnerable road users, and policymakers. This work introduces\na novel human reasons-based supervision framework that\ndetects when A V behaviour misaligns with expected human\nreasons to trigger trajectory reconsideration. The framework\nintegrates with motion planning and control systems to support\nreal-time adaptation, enabling decisions that better reflect\nsafety, efficiency, and regulatory considerations. Simulation\nresults demonstrate that this approach could help A Vs respond\nmore effectively to ethical challenges in dynamic driving envi-\nronments by prompting replanning when the current trajectory\nfails to align with human reasons. These findings suggest that\nour approach offers a path toward more adaptable, human-\ncentered decision-making in A Vs.\nI. I NTRODUCTION\nAddressing the ethical complexities that emerge in daily\ndriving contexts remains essential for social acceptance of\nautomated vehicles (A Vs). Despite their promised advantages\nin safety improvements and transportation access [1], the\nwidespread adoption of these systems hinges on their capac-\nity to reflect human ethical judgment, particularly when con-\nfronting morally ambiguous situations where multiple values\ncompete [2], [3]\u2014situations commonly referred to as ethical\ndilemmas. Examples include deciding whether to briefly\noccupy the opposite lane to safely overtake cyclists [4],\nor speeding up temporarily to avoid unsafe situations. This\nbehaviour reveals a critical gap in A V decision-making: the\nnecessity of designing A Vs capable of dynamically balancing\nmultiple considerations, such as safety, efficiency, regulatory\ncompliance, and contextual appropriateness, in real-time,\nrather than relying solely on predefined regulations.\nCurrent approaches show limitations when it comes to\nhandling everyday ethical dilemmas in automated driving.\n1Department of Transport and Planning,2Department of\nCognitive Robotics,3Centre for Meaningful Human Control,\nDelft University of Technology, 2628 CN Delft, The Netherlands.\nl.e.Suryana@tudelft.nl; s.rahmani@tudelft.nl;\ns.c.calvert@tudelft.nl; b.vanarem@tudelft.nl;\na.zgonnikov@tudelft.nl.Much of the research has focused on extreme scenarios, such\nas the well-known \u201ctrolley problem\u201d [5]. While philosoph-\nically significant, trolley problems rarely happen in daily\ndriving, and despite the practical importance of everyday\ndilemmas, they often receive less attention [1], [6]. [2] em-\nphasises that everyday ethical decisions in automated driving\nextend far beyond these extreme scenarios, requiring nuanced\ncontextual use of reasons, something current systems lack.\nSimilarly, [7] argue that framing A V ethics as simplified\ntrolley problems fails to capture the probabilistic nature and\ndynamic complexity of real-world driving situations.\nAlthough the ethical dimensions of \u201cmundane\u201d scenarios\nmay seem straightforward for human drivers, they require\ncontext-aware judgment that comes naturally to human but\nposes challange for A Vs. These judgments must balance\nmultiple ethically relevant considerations, such as safety,\nefficiency, and social norms. This adaptability represents a\nchallenge for A V systems designed with traditional motion\nplanning algorithms, which primarily optimise for trajec-\ntory smoothness and collision avoidance without explicitly\nintegrating ethical considerations. Building on this under-\nstanding, recent ethical frameworks propose more holistic\napproaches that better align with human moral intuitions. [8]\nand [9] collectively emphasise that effective A V ethics must\nintegrate moral principles such as deontological ethics, virtue\nethics, and consequentialist considerations while maintaining\ntransparency in decision-making.\nHowever, integrating these ethical principles into A V\ndecision-making remains challenging. While prior works\nhave focused on embedding such principles into control and\nmotion algorithms [1], [10], [11], current approaches fail to\nmake explicit when these principles are in conflict. Recog-\nnising these conflicts is essential for enabling transparent\ndecisions and for adjusting A V behaviour to better reflect\nthe ethical principles the system is intended to uphold.\nThe concept of meaningful human control (MHC) [12],\n[13] offers a promising conceptual bridge for addressing\nthe challenge of making explicit which moral principles are\nin conflict. MHC asserts that humans should ultimately be\nresponsible for every decision made by automated systems.\n[13] laid the groundwork for achieving MHC. One of the\nrequired conditions is the tracking condition, which requires\nautomated systems to respond to the reasons of relevant\nhumans. In the remainder of this paper, we refer to these\nrelevant humans \u2013 such as drivers, passengers, pedestrians,\nand policymakers \u2013 as stakeholders. According to [12], these\nreasons can be understood as moral values or principles that\nare reflected in human driving plans and intentions\u2014such asarXiv:2507.23308v1  [eess.SY]  31 Jul 2025\n\n--- Page 2 ---\nensuring safety and comfort for both themselves and others,\ndriving efficiently, and complying with traffic regulations.\nFrom this perspective, if an A V is designed to uphold certain\nmoral principles, the tracking condition provides a clear\nexpectation that its behaviour should reflect corresponding\nhuman plans and intentions.\nTo operationalise this concept and address the challenges\nof handling ethical dilemmas and making moral principles\nexplicit in A V decision-making, we propose a novel human\nreasons-based supervision framework that enables A Vs to\nevaluate if their behaviour aligns with the reasons of diverse\nstakeholders. By grounding this framework in the tracking\ncondition of meaningful human control, we aim to support\nA V decision-making in ethically challenging everyday sce-\nnarios that require balancing multiple, sometimes conflicting,\nvalues.\nSpecifically, the primary contribution of this paper is a\nmodular human reasons-based supervision framework that\nenables A Vs to make ethically nuanced decisions in routine\nyet ethically challenging scenarios. The framework contin-\nuously evaluates how well the A V\u2019s behaviour aligns with\nhuman reasons and triggers replanning when a misalignment\nis detected. The paper contributes:\n1) We developed a detection mechanism that uses stake-\nholder reason scores and predefined thresholds to iden-\ntify when A V behaviour misaligns with human reasons;\n2) We integrated the human reasons-based supervision\nframework into an A V control architecture, including\na mechanism for triggering replanning when reason\nscores fall below predefined thresholds;\n3) We enabled explainability by using reason scores as\ninterpretable indicators of why behaviour changes are\nrecommended in routine, ethically challenging situa-\ntions.\nThis work advances the discourse on A V decision-making\nin ethically challenging transportation scenarios by bridging\nthe gap between moral principles and practical A V decision-\nmaking, ultimately supporting the development of socially\nacceptable automated mobility solutions that align with hu-\nman reasons across diverse everyday scenarios.\nThe remainder of this paper is organised as follows:\nSection II presents the detailed methodology and system\narchitecture, including the mathematical human reasons and\nits integration into a motion planning framework. Section III\ndescribes the experimental setup and simulation environment.\nSection IV and V present and discuss the simulation results\nand the impact of ethical supervision on vehicle behaviour.\nFinally, Section VI concludes the paper and outlines direc-\ntions for future research.\nII. M ETHODOLOGY\nA. Problem Formulation\nWe formalise the automated vehicle navigation problem\nin scenarios involving ethical decision-making. While the\nframework is generic and applicable to a wide range of\nsituations, for demonstration purposes, we consider a sce-\nnario including the interaction of an automated vehiclewith a vulnerable road user (VRU). In this scenario, an\nautomated vehicle navigating a bidirectional road faces an\nethical dilemma during overtaking manoeuvres. To maintain\nefficient travel, the vehicle must either follow the VRU with\na very low speed, which is not desirable for the vehicle\u2019s\npassenger, or overtake a slower-moving VRU, which may\nrequire temporarily entering the oncoming lane or reducing\nthe safety buffer with the cyclist. This manoeuvre challenges\nforces a trade-off between strict compliance, user safety, and\ntravel efficiency.\nFor the problem formulation, we consider the automated\nvehicle operating in state space X \u2282Rnwith state vector\nxt= [pt, vt, \u03b8t, \u03c9t]T, where pt= [px, py]Trepresents\nposition, vtdenotes velocity, \u03b8tis heading angle, and \u03c9t\nis rotational velocity. The control space U \u2282Rmconsists of\nut= [at, \u03b4t]T, representing acceleration and steering angle.\nOur multi-agent ethical framework defines stakeholders\nS={s1, s2, ..., s k}with reason functions Rsi:X \u00d7\nU \u2192 [0,1]quantifying alignment with each stakeholder\u2019s\nperspective. This formulation means that each function Rsi\nevaluates the vehicle\u2019s state and control actions to produce\na score between 0 and 1, reflecting how well the A V\u2019s\nbehaviour satisfies the ethical priorities of the respective\nstakeholder. For the designed scenario, we identify three key\nstakeholders: road policymakers ( spolicy ), vulnerable road\nuser ( sV RU ), and drivers ( sdriver ).\nThe navigation problem for the automated vehicle is\nformulated as:\nmin\nu0,...,u T\u22121T\u22121X\nt=0J(xt,ut)\ns.t. xt+1=f(xt,ut)\nxt\u2208 Xsafe\nut\u2208 U\nRsi(xt,ut)\u2265\u03c4si,\u2200si\u2208 S(1)\nHere,J(xt,ut)represents the cost function to be min-\nimised over the control horizon T, evaluating the perfor-\nmance of the A V\u2019s state and control inputs at each time step t.\nThe function f(xt,ut)denotes the system dynamics model\nthat predicts the next state xt+1based on the current state\nxtand control input ut. The set Xsafe\u2282 X defines the safe\nregion of the state space where the vehicle must operate\nto avoid collisions and other hazards. Finally, for each\nstakeholder si,\u03c4sidenotes the threshold value specifying the\nminimum acceptable reason score that the A V\u2019s behaviour\nmust meet.\nB. Framework Architecture\nOur approach implements a multi-component and hierar-\nchical framework with three main components:\n1)Global Motion Planning : Responsible for finding a\nreference trajectory for the vehicle to be followed.\nIt uses A* search with motion primitives to generate\nfeasible reference paths from the current state of the\nvehicle to the goal location.\n\n--- Page 3 ---\n2)Model Predictive Control : Optimises vehicle trajec-\ntory when following the reference path. It ensures\nkinodynamic feasibility and satisfying soft and hard\nconstrained defined, such as safety, efficiency, and\ncomfort.\n3)Human Reasons-based Supervision Framework : Eval-\nuates the planned actions against ethical criteria and\ntriggers replanning when necessary if the criteria are\nnot met.\nThese elements are depicted in Fig. 1. The key innovation\nin our approach is the definition and integration of a human\nreasons-based supervision framework as a mechanism for\ntriggering replanning, ensuring that the vehicle\u2019s behaviour\nsatisfies ethical constraints derived from multiple stake-\nholders\u2019 perspectives. Therefore, we begin by detailing the\ncomponents of the framework.\nC. Human Reasons-based Supervision Framework\nOur approach to developing a framework that supervises\nthe alignment between A V behaviour and human reasons\nbuilds on the qualitative evaluation steps for the tracking\ncondition outlined by [14]. These steps involve defining the\nrelevant stakeholders and articulating their reasons, as well\nas specifying the features of the A V system that govern its\nbehaviour. While the original approach remains qualitative,\nour work extends it by developing a quantitative framework.\nSpecifically, after identifying the stakeholders, we formalise\ntheir reasons mathematically. This process is described in\nthis section, while the features of the A V system that govern\nits behaviour are presented in Section II-D.\n1) Identification of Stakeholders and Reason Models: To\neffectively integrate human ethical considerations into A V\ndecision-making, it is essential to identify the key stakehold-\ners involved and define how system\u2019s behaviour influence\ntheir alignment with stakeholders\u2019 reasons. Accordingly, we\nmodel three primary stakeholders in the designed scenario:\n\u2022Road Policymaker : Represents regulatory authorities\nwhose reason is to ensure overall road safety through\nregulatory compliance.\n\u2022Vulnerable Road User : Represents vulnerable road users\nwhose reason is to commute with safety and comfort.\n\u2022Driver : Represents the vehicle occupant\u2019s motivation\nfor efficiency and arriving at the destination as fast as\npossible.\nTo operationalise stakeholder perspectives within our\nframework, we establish specific reason models for each. For\neach stakeholder, we define a reason model that quantifies\ntheir satisfaction with the vehicle\u2019s behaviour on a scale from\n0 to 1, where 1 indicates full satisfaction and 0 indicates\ncomplete dissatisfaction.\n2) Mathematical Representation of Reason Models: A\nkey contribution of our framework is the operationalisation of\nabstract human reasons using empirically supported, measur-\nable parameters. While previous work has discussed human\nreasons conceptually [14] or proposed initial variables [15],\nthese efforts have not established an empirically grounded\nmapping to real-world driving variables. We address this gapby introducing a reason model grounded in human factors\nresearch.\nTo translate stakeholder\u2019s reason into a computationally\nviable form, we adopt a set of piecewise exponential func-\ntions that model how stakeholder satisfaction declines when\nspecific behavioural threshold are crossed. These modelling\nchoice are grounded based on both computational simplic-\nity and their ability to approximate human reasons. As\ndemonstrated by [16], individuals tend to overweight low\nprobabilities and underweight high probabilities, implying\na rapid shift in perceived risk once certain thresholds are\ncrossed. Thus, the exponential function is chosen because it\ncan well capture the rapid change in perceived acceptability.\nNevertheless, the proposed equations serve as a representa-\ntive model that can be adjusted via thresholds and scaling\nconstants to suit various scenarios.\nTo ensure realism, each stakeholder\u2019s reason is modelled\nusing scenario-specific variables informed by human factors\nresearch. Details of the experimental case appear in Sec-\ntion III. For example, the cyclist\u2019s reason\u2014related to comfort\nand perceived safety\u2014is represented using lateral distance\nand tailgating time, based on findings from [17], [18]. Here,\ncomfort refers to the cyclist\u2019s subjective experience of emo-\ntional and physical ease during interaction with the vehicle.\nEmpirical studies show that insufficient lateral clearance and\nprolonged close following increase stress and perceived risk,\njustifying the use of these variables as proxies.\nThe same vehicle behaviour\u2013prolonged following\u2013also im-\npact driver\u2019s reason, which relates to driving efficiency. This\nis modelled as perceived impatience, based on time and\ndistance the A V follows the cyclist at close range. As shown\nin [19], such conditions could lead to frustration under time\npressure. Meanwhile, the policymaker\u2019s reason\u2014regulatory\ncompliance\u2014is satisfied when the A V stays within its desig-\nnated lane. The reason\u2019s score decreases as the A V crosses\ninto the opposite lane to overtake the cyclist. This reflects\nfindings from [20], where experts preferred the A V to over-\ntake gently and return to its lane promptly, indicating that\nin such situations, any lane violation should be considered\nminimal.\na) Policymaker\u2019s Reason: The policymaker\u2019s reason\nscore quantifies regulatory compliance \u2013 specifically, adher-\nence to lane regulations in this scenario:\nRpolicymaker =(\n1, ifdveh>0,\nek1\u00b7dveh,otherwise ,(2)\nwhere dvehis the lateral displacement of the ego vehicle\nfrom the center line (positive when on the correct side of the\nroad, negative when in the oncoming lane); k1is a scaling\nconstant.\nb) VRU\u2019s Reason: The VRU\u2019s reasons score is decom-\nposed into safety assurance and comfort preservation.\n-Safety Assurance:\nRsa(t) =(\n1, ifdveh-vru > d th,vru,\n1\nek2(dveh-vru\u2212dth,vru),otherwise ,(3)\n\n--- Page 4 ---\nEnvironment Perception and State EstimationHuman Reasons-based SupervisionStakeholder 1: !!:#\"!$#,&#Stakeholder 2: !$:#\"\"$#,&#Stakeholder N: !%:#\"#$#,&#Reason-Triggered Re-plannermin!!\u2208#$!!%$,'$<\t*!!\t\u2200,%\u2208.Hierarchical Control ArchitectureGlobal Motion PlannerLocal Motion Controller (MPC)/=\t/$&'()+\t/(*+$&*,+/!-**$.+/$/&-%+',Acceleration + SteeringReplanning Needed?\u2026\nYesNoExecute Acceleration and SteeringFig. 1. Hierarchical control architecture with human reasons-based supervision for ethical A V decision-making\nwhere dveh-vru denotes the distance between the vehicle and\nthe VRU; dth,vru is the perceived safe distance threshold; k2\nis a scaling constant.\n-Comfort Preservation:\nRcp(t) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f31, iftclose,vru < t th,vru or\ndveh-vru > d th,vru,\n1\nek3(tfollow\u2212tth,vru),otherwise .(4)\nwhere tclose,vru is the cumulative time during which\ndveh-vru < d th,vru;tth,vru is the maximum tolerable time for the\ncyclist to be followed too closely; k3is a scaling constant.\nThe overall VRU\u2019s reason score is then given by:\nRVRU(t) =Rsa(t)\u00b7Rcp(t). (5)\nwhere RVRU(t)combines the safety and comfort compo-\nnents.\nc) Driver\u2019s Reason: The driver\u2019s reason score is de-\nfined as:\nRdriver(t) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f31, iftbehind,driver < t th,driver or\ndveh-vru > d th,driver ,\n1\nek4(tbehind,driver\u2212tth,driver),otherwise ,\n(6)\nwhere tbehind,driver is the cumulative time during which\ndveh-vru < d th,driver ;tth,driver is the time threshold for close\nfollowing that the driver considers acceptable; dveh-vru is the\ndistance between the vehicle and the VRU; dth,driver is the\ndistance threshold below which the driver considers the A V\nto be following too closely, leading to perceived inefficiency;\nandk4is a scaling constant. Note that the scores of k1, k2, k3,\nandk4= 0.2in our experiment can be adjusted depending\non how quickly we want the reasons to shift from 1 to 0\nwhen the reason thresholds are crossed.\nD. Motion Planning and Control Implementation\nTo demonstrate the generalisability and practical imple-\nmentability of our framework, we integrate the humanreason-based supervision framework into an A V feature\nthat governs its behaviour. In this research, we adopt an\nestablished motion planning and control framework [21].\nIn the following, we briefly describe the underlying motion\nplanning and control mechanism and explain how the reason-\ntriggered replanning seamlessly fits into this structure.\n1) Motion Planning: The motion planner in this study\nbuilds a directed graph from the vehicle\u2019s current state using\npre-computed motion primitives. An A* algorithm is applied\nto find the optimal path by minimizing the cost function\nJpath=w1\u00b7Jlength+w2\u00b7Jsmoothness\n+w3\u00b7Jobstacle clearance +w4\u00b7Jtraffic rule (7)\nwhere Jlength is the cost related to the length of the path\nfrom the initial state to the goal state, aiming to motivate the\nshortest path; Jsmoothness is the cost related to the smoothness\nof the path; Jobstacle clearance is the cost for avoiding obstacles;\nandJtraffic ruleis the cost aiming to avoid areas prohibited\nby traffic rules. The output of the planner is a reference\ntrajectory passed to the controller for execution. We employ\na modified version of A* to enhance search efficiency and\napplicability for our specific use case. The detailed algorithm\nimplementation is documented in [22]. It\u2019s worth noting that\nthe cost function has been slightly modified to fit the purpose\nof this study. More specifically, the weights related to the\ncosts for obstacle clearance and traffic adherence have been\nseparated compared to the standard implementation in [22].\n2) Controller: We formulate a finite-horizon optimisation\nproblem that is solved at each time step to ensure trajectory\nfollowing while respecting user-defined constraints.\nThe vehicle state at the time step tis represented as:\nx(t) = [xt, yt, \u03b8t, vt]T(8)\ncomprising position coordinates (xt, yt), heading angle \u03b8t,\nand longitudinal velocity vt. The control inputs are:\nu(t) = [at, \u03b4t]T(9)\n\n--- Page 5 ---\nwhere atdenotes acceleration and \u03b4tthe steering angle.\nVehicle dynamics are modelled using a bicycle model as\nfollows:\n\u02d9x=vcos(\u03b8+\u03b2),\u02d9y=vsin(\u03b8+\u03b2),\u02d9\u03b8=v\nLsin(\u03b2),\u02d9v=a\n(10)\nwith slip angle \u03b2= arctan(lr\nLtan(\u03b4)), wheelbase L, and\nrear axle distance lr. We discretise this continuous model\nusing time step Ts:\nx(t+ 1) = Adx(t) +Bdu(t) +dd (11)\nwhere the discrete system matrices are:\nAd=\uf8ee\n\uf8ef\uf8ef\uf8f01 0 Tsc\u03b8\u2212Tsvts\u03b8\n0 1 Tss\u03b8 Tsvtc\u03b8\n0 0 1 0\n0 0Tstan(\u03b4t)\nL1\uf8f9\n\uf8fa\uf8fa\uf8fb(12)\nwithc\u03b8= cos( \u03b8t)ands\u03b8= sin( \u03b8t)for brevity. The input\nmatrix and disturbance term are:\nBd=\uf8ee\n\uf8ef\uf8ef\uf8f00 0\n0 0\nTs 0\n0Tsvt\nLcos2(\u03b4t)\uf8f9\n\uf8fa\uf8fa\uf8fb(13)\ndd=\uf8ee\n\uf8ef\uf8ef\uf8f0Tsvts\u03b8\u03b8t\n\u2212Tsvtc\u03b8\u03b8t\n0\n\u2212Tsvt\u03b4t\nLcos2(\u03b4t)\uf8f9\n\uf8fa\uf8fa\uf8fb(14)\nOur cost function integrates multiple objectives over pre-\ndiction horizon N:\nJ=N\u22121X\nt=0(\u2225e\u22a5\nt\u22252\nQ\u22a5+\u2225e\u2225\nt\u22252\nQ\u2225+\u2225e\u03b8v,t\u22252\nQ\u03b8v)\n+N\u22121X\nt=0(\u2225ut\u22252\nR+\u2225\u2206ut\u22252\nRd)\n+\u2225xN\u2212xref,N\u22252\nQf(15)\nwhere e\u22a5\ntande\u2225\ntrepresent perpendicular and parallel tra-\njectory tracking errors, e\u03b8v,tcaptures orientation and velocity\nerrors, and \u2206ut=ut+1\u2212ut. The matrices Q\u22a5,Q\u2225,Q\u03b8v,R,\nRd, and Qfare weighting matrices that prioritize different\naspects of performance.\nThe optimisation operates under constraints:\nxt+1=f(xt,ut);ut\u2208 U;xt\u2208 X (16)\nwhere Udefines input limitations:\namin\u2264at\u2264amax;\u03b4min\u2264\u03b4t\u2264\u03b4max (17)\n3) Reason-Triggered Replanning: At each time step, the\nsystem evaluates the reason scores for all stakeholders using\nthe formulations provided in Eq. 2 to Eq. 6. For each stake-\nholder si\u2208 S, the reason score Rsi(xt,ut)is computed. If\nany score falls below its corresponding threshold \u03c4si,\nmin\nsi\u2208SRsi(xt,ut)< \u03c4si, (18)\nCyclistEgo vehicleRoad infrastructureCyclist\u2019s pathEgo Vehicle\u2019s pathFig. 2. Illustration of an ego vehicle approaching a cyclist on a narrow\nbidirectional road, highlighting the ethical challenge in overtaking due to\nroad constraints.\nthe system immediately triggers a replanning cycle. During\nreplanning, the current scenario is updated, and a new\nreference trajectory is generated and passed to the controller.\nThis continuous evaluation ensures that the vehicle\u2019s motion\nremains aligned with the ethical and performance criteria\nof all stakeholders. In our implementation, the path finding\nalgorithm incorporates a set of weights in its cost function\n(Eq. 7) to provide flexibility when replanning is needed. For\ninstance, under normal circumstances, prohibited areas by\ntraffic rules are treated similarly to obstacles by assigning\nlarge weights to the Jtraffic rule, restraining the A* search\nalgorithm from generating paths through those areas. When\nreplanning is triggered due to misalignment with human rea-\nsons, prohibited areas could temporarily receive lower costs,\nallowing the A* algorithm to search through those areas and\nprovide a new trajectory with a different, potentially higher,\nhuman-reasoning score. Since the replanning strategy is not\nwithin the scope of this study, we refer the readers to the im-\nplementation of our planner detailed in [22]. We would like\nto highlight that the proposed evaluation framework remains\nalgorithm-agnostic and can assess trajectories generated by\nany motion planning approach.\nIII. EXPERIMENT SETUP\nTo evaluate our human reasons-based supervision frame-\nwork, we test it in an ethically challenging cyclist overtaking\nscenario, where an ego vehicle traveling in the right lane\nencounters a slow-moving cyclist on a narrow road (Fig. 2).\nSafely overtaking requires the vehicle to briefly enter the left\nlane, which is normally reserved for oncoming traffic. This\nforces a trade-off between strict lane adherence and efficient,\nsafe manoeuvring, highlighting the ethical dilemma arising\nfrom the conflicting priorities of the involved stakeholders:\n\u2022Road Policymakers enforce traffic regulations that pro-\nhibit left-lane usage to ensure overall road safety.\n\u2022Cyclists require a safe and comfortable riding experi-\nence, which may be compromised by vehicles manoeu-\nvring too closely.\n\u2022Drivers aim for efficient travel, potentially pressuring\nthe system to overtake despite the inherent safety and\nregulatory concerns.\nThe detailed definitions of the reason models for each\n\n--- Page 6 ---\nstakeholder are provided in Eq. 2 to Eq. 6. In our experi-\nments, we focus on comparing two configurations:\n\u2022Baseline Controller: The ego vehicle operates using a\nstandard baseline controller without the human reasons-\nbased supervision [21].\n\u2022Baseline Controller with Replanner: The baseline con-\ntroller is augmented with the human reasons-based\nsupervision framework, which triggers replanning when\nthe vehicle\u2019s behaviour does not align with the prede-\nfined ethical thresholds.\nThese experiments are designed to assess how integrat-\ning human reasons-based supervision framework impacts\ndecision-making in vehicle behaviour during ethically chal-\nlenging situations; specifically in the context of safely over-\ntaking a cyclist on a bidirectional road. To calibrate our\nreason models, we set the threshold values summarised in\nTable I based on empirical studies of cyclist and driver\nbehaviour [18], [23].\nTABLE I\nPARAMETER VALUES FOR REASON MODELS\nParameter Value\ndth,vru (Cyclist\u2019s perceived too-close distance threshold) 8 m\ntth,vru (Max time cyclist tolerates close following) 5 s\ndth,driver (Driver\u2019s perceived too-close distance threshold) 12 m\ntth,driver (Max time driver tolerates close following) 10 s\n\u03c4si(Reason alignment threshold for all stakeholders) 0.7\nIV. RESULTS\nThe results of validation for the controller with and with-\nout the proposed human reasons-based supervision frame-\nwork are depicted in Fig. 3 and Fig. 4, respectively. The\ntimestamps next to the ego vehicle and the cyclist indicate\ntheir positions at that time, helping to visualize their relative\nmovements. The results for the Baseline controller suggest\nthat while the system successfully handles basic path plan-\nning and collision avoidance, it does not adequately account\nfor human reasons, particularly in terms of the driver\u2019s and\ncyclist\u2019s perspectives. As shown in Fig. 3.a, the ego vehicle\nfollows the cyclist and reaches the goal in 35 seconds without\nattempting to overtake. This behaviour demonstrates a stop-\nand-go dynamic, which is further illustrated in Fig. 3.c,\nwhere the speed of the ego vehicle is depicted. Initially,\nthe global planner generates a smooth path toward the goal.\nHowever, when the ego vehicle approaches the cyclist and\na potential collision risk arises, the controller activates a\ncollision avoidance strategy, reducing the ego vehicle\u2019s speed\nto avoid the potential collision. As the distance between the\ntwo increases, the controller allows the vehicle to accelerate\nand realign with the planned path.\nDespite effectively tracking the planned trajectory (Fig.\n3.d), the system\u2019s performance in aligning with human\nreasons decreases over time (Fig. 3.b). It is apparent that\nfrom the 10th second until the end of the simulation, the\ndriver\u2019s reason score for time efficiency decreases sharply to\nzero by the end of the simulation. This is because the driver\nmust remain patient to stay in the mode of following the\nFig. 3. Results of running the model in baseline controller\nvehicle from behind. On the other hand, the cyclist\u2019s reason\nscore for comfort fluctuates (due to the fluctuations of the\nvehicle\u2019s speed and distance to the cyclist) but ultimately\nforms a decay pattern. Over time, the score decreases further\ndue to the accumulation of time spent being followed by\nthe ego vehicle at a close distance. Meanwhile, the system\u2019s\nperformance demonstrates strong alignment with the road\npolicymaker\u2019s reason score for regulatory compliance since\nthe ego vehicle consistently stays in the right lane.\nThe Baseline controller with a replanner allows the ego\nvehicle to successfully overtake the cyclist, addressing hu-\nman reason priorities but at the cost of temporary regula-\ntory compliance violations. Initially, the ego vehicle follows\nthe cyclist from behind for the first 11 seconds, adhering\nto its straight path trajectory while exhibiting stop-and-go\nbehaviour, as shown in Fig. 4.c. At the 11.5-second mark, the\nhuman reason-based supervision framework detects that the\ndriver\u2019s reason score for time efficiency has fallen below its\nthreshold of 0.7, due to the accumulation of the waiting time\nof the driver and the cyclist. This triggers the planner to gen-\nerate a new feasible trajectory. This new path briefly crosses\nthe bidirectional road before returning to the right lane to\nreach the goal. During the overtaking manoeuvre, the close\nproximity between the ego vehicle and the cyclist causes a\ntemporary decrease in reason scores, and the violation of the\nright-lane regulation further reduces the policymaker\u2019s reason\nscore. However, once the ego vehicle successfully overtakes\nthe cyclist and returns to the intended lane, all reason values\nrecover to one. In this scenario, the ego vehicle achieves the\n\n--- Page 7 ---\nFig. 4. Results of running the model in baseline controller with replanner\ngoal in just 18 seconds by overtaking the cyclist, significantly\nreducing the driver\u2019s waiting time. It is worth noting that the\ndeviations on the order of centimeters in both scenarios may\nbe attributed to the limitations and constraints of the MPC.\nV. DISCUSSION\nThis study reveals three key contributions of the human\nreasons-based supervision framework: (1) the ability to de-\ntect when current system behaviour no longer aligns with\nthe priorities of human stakeholders by monitoring reason\nscore againts adaptive thresholds, (2) the modularity of the\nframework to adapt controller behaviour without majorly\nmodifying core components like the global planner or MPC\nsettings, and (3) the inherent explainability of decision-\nmaking processes, enabling autonomous systems to justify\nbehavioural changes based on stakeholder reason alignment.\nThese features are essential for building trust and acceptance\nin automated vehicle deployment. The experimental results\nyield several insights. While both the Baseline Controller\nand the Baseline Controller with Replanner successfully\nguide the ego vehicle to follow the planned trajectory and\nreach the goal area, they differ significantly in how they\nrespond to human reasons. This leads to distinct trade-offs\nin performance and alignment with the priorities of human\nreasons.\nThe Baseline Controller demonstrates a conservative ap-\nproach, prioritizing the policymaker\u2019s reason for regulatory\ncompliance. This results in strict adherence to rules, but at the\ncost of neglecting other human reasons priorities. While the\ncontroller achieves basic objectives like collision avoidanceand goal attainment, it fails to address the driver\u2019s reason for\ntime efficiency and the cyclist\u2019s reason for comfort.\nIn contrast, the Baseline Controller with Replanner intro-\nduces a dynamic and adaptive approach through the human\nreasons-based supervision component of the framework. Our\nresults show that responding to triggers can lead to decisions\nthat better reflect a balance of human reasons. For example,\nthe system may temporarily violate regulatory compliance\n(lowering the policymaker\u2019s reason score) to reduce discom-\nfort for the cyclist or impatience from the driver. While the\ntrigger does not resolve value conflicts, it signals when the\ncurrent trajectory may no longer align with a stakeholder\u2019s\nreasons. However, how the planner could systematically se-\nlect among alternatives is beyond the scope of this research.\nFuture work is needed to extend this supervision layer with\ndecision-making mechanisms that actively weigh and decide\nhow to respond when human reasons are in conflict.\nThe choice of threshold values plays a critical role in this\nframework. A lower threshold might delay intervention, lead-\ning to prolonged misalignment with human reasons, while a\nhigher threshold could result in overly frequent replanning,\nwhich increases the computational cost. Additionally, the\nvehicle\u2019s state when the reasons falls below the threshold\u2014\nsuch as its proximity to the cyclist, speed, or surrounding\nenvironment\u2014can influence the feasibility of the replanned\ntrajectory. These factors highlight the importance of carefully\npick the right threshold to ensure feasibility and stability.\nHowever, while the threshold offers an interpretable mech-\nanism for initiating replanning, it does not capture the full\nnuance of how human drivers make context dependent trade-\noffs, such as deciding when it is safe to pass with oncoming\ntraffic or assessing visibility in hilly terrain. Rather than\nprescribing the best course of action, the framework uses\nthresholds to signal that the current plan may no longer\nreflect certain stakeholder priorities. Grounding threshold\nvalues in empirical studies, and learning or tuning them from\nhuman data, is a promising direction for future work.\nNonetheless, we emphasise that our human reasons-based\nsupervision framework, which triggers replanning based on\nthreshold values, is not intended to compete with existing\nnuanced decision-making algorithms for dynamic environ-\nments, such as multi-policy decision-making (MPDM) pro-\nposed by [24], [25], but to complement them. While our\nframework has lower resolution than MPDM\u2019s continuous\npolicy evaluation, its strength lies in simplicity. It avoids the\ncomputational cost of constant replanning by activating only\nwhen a misalignment with human reasons is detected.\nFuture work could integrate MPDM-style approaches into\nour architecture, enabling motion planners that not only gen-\nerate but also evaluate candidate trajectories based on human\nreasons rather than predefined policies. This integration could\nsupport more nuanced balancing of stakeholder priorities\nwhile preserving interpretability.\nOverall, by aligning system behaviour with human prior-\nities, the proposed framework enables more human-centric\ndecision-making, which is essential for user trust and accep-\ntance in real-world applications. Thanks to the framework\u2019s\n\n--- Page 8 ---\nmodularity, its integration into existing automated system\narchitectures is straightforward. Its implementation can be\nextended to more complex environments, such as urban\ndriving or multi-agent systems, where balancing multiple\nreasons priorities is critical. Future work could explore\nenhancing the framework\u2019s capabilities, such as using it not\nonly to trigger replanning but also to identify trajectories\nthat maximise human reasons across all agents. Testing\nin dynamic and unpredictable environments would further\nvalidate its robustness and scalability.\nThis study underscores the importance of incorporating\nhuman reason into automated driving systems. The findings\ndemonstrate that while strict regulatory compliance ensures\nsafety and rule-following, mechanisms that detect misalign-\nment with human priorities and prompt reconsideration can\nlead to decisions that better reflect the reasons of multiple\nstakeholders. This insight paves the way for future develop-\nments in automated systems that are both technically robust\nand socially and ethically aligned with human reasons.\nVI. CONCLUSION\nThis study proposes a human based-reason supervision\nframework to support automated vehicles (A Vs) to navigate\nroutine yet ethically challenging scenarios. The framework\nintroduces a novel approach to A V planning by evaluating\nwhether the vehicle\u2019s behaviour aligns with human reason\nand triggering a replan assignment if misalignment is de-\ntected. The key contributions demonstrated through this work\nare: (1) A detection mechanism for identifying misalignment\nbetween A V behaviour and stakeholder reasons based on\nreason score thresholds; (2) Modular integration into the\nA V control architecture without modification of the core\nplanner or motion controller; (3) Explainability through\nthe use of stakeholder reason scores, enabling interpretable\njustifications for behavioural changes. These features enable\nA Vs to align with human reasons in real time, ensuring more\nhuman-centric decision-making.\nVII. ACKNOWLEDGMENTS\nWe declare that OpenAI ChatGPT was used in the writing\nprocess to enhance the style and conciseness of the text\noriginally written by the authors. This work was funded by\nthe Indonesia Endowment Fund for Education (LPDP) under\nGrant 0006552/TRA/D/19/lpdp2021.\nREFERENCES\n[1] M. Geisslinger, F. Poszler, J. Betz, C. L \u00a8utge, and M. Lienkamp,\n\u201cAutonomous driving ethics: From trolley problem to ethics of risk,\u201d\nPhilosophy & Technology , vol. 34, no. 4, pp. 1033\u20131055, 2021.\n[2] P. Lin, \u201cWhy ethics matters for autonomous cars,\u201d Autonomous driv-\ning: Technical, legal and social aspects , pp. 69\u201385, 2016.\n[3] J. Millar, P. Lin, K. Abney, and G. Bekey, \u201cEthics settings for\nautonomous vehicles,\u201d Robot ethics , vol. 2, pp. 20\u201334, 2017.\n[4] Unknown, \u201cTesla following a cyclist,\u201d 2025, accessed: 2025-03-01.\n[5] J.-F. Bonnefon, A. Shariff, and I. Rahwan, \u201cThe trolley, the bull bar,\nand why engineers should care about the ethics of autonomous cars\n[point of view],\u201d Proceedings of the IEEE , vol. 107, no. 3, pp. 502\u2013\n504, 2019.\n[6] J. Himmelreich, \u201cNever mind the trolley: The ethics of autonomous\nvehicles in mundane situations,\u201d Ethical Theory and Moral Practice ,\nvol. 21, no. 3, pp. 669\u2013684, 2018.[7] S. Nyholm and J. Smids, \u201cThe ethics of accident-algorithms for self-\ndriving cars: An applied trolley problem?\u201d Ethical theory and moral\npractice , vol. 19, no. 5, pp. 1275\u20131289, 2016.\n[8] D. Cecchini, M. Pflanzer, and V . Dubljevi \u00b4c, \u201cAligning artificial intelli-\ngence with moral intuitions: An intuitionist approach to the alignment\nproblem,\u201d AI and Ethics , pp. 1\u201311, 2024.\n[9] A. Henschke, \u201cTrust and resilient autonomous driving systems,\u201d Ethics\nand Information Technology , vol. 22, no. 1, pp. 81\u201392, 2020.\n[10] S. M. Thornton, S. Pan, S. M. Erlien, and J. C. Gerdes, \u201cIncorpo-\nrating ethical considerations into automated vehicle control,\u201d IEEE\nTransactions on Intelligent Transportation Systems , vol. 18, no. 6, pp.\n1429\u20131439, 2016.\n[11] M. Geisslinger, F. Poszler, and M. Lienkamp, \u201cAn ethical trajectory\nplanning algorithm for autonomous vehicles,\u201d Nature Machine Intel-\nligence , vol. 5, no. 2, pp. 137\u2013144, 2023.\n[12] G. Mecacci and F. Santoni de Sio, \u201cMeaningful human control as\nreason-responsiveness: the case of dual-mode vehicles,\u201d Ethics and\nInformation Technology , vol. 22, no. 2, pp. 103\u2013115, 2020.\n[13] F. Santoni de Sio and J. Van den Hoven, \u201cMeaningful human control\nover autonomous systems: A philosophical account,\u201d Frontiers in\nRobotics and AI , vol. 5, p. 323836, 2018.\n[14] L. E. Suryana, S. Nordhoff, S. Calvert, A. Zgonnikov, and B. van\nArem, \u201cMeaningful human control of partially automated driving\nsystems: Insights from interviews with tesla users,\u201d Transportation\nResearch Part F: Traffic Psychology and Behaviour , vol. 113, pp. 213\u2013\n236, 2025.\n[15] S. C. Calvert and G. Mecacci, \u201cA conceptual control system descrip-\ntion of cooperative and automated driving in mixed urban traffic with\nmeaningful human control for design and evaluation,\u201d IEEE Open\nJournal of Intelligent Transportation Systems , vol. 1, pp. 147\u2013158,\n2020.\n[16] A. Tversky and D. Kahneman, \u201cAdvances in prospect theory: Cumu-\nlative representation of uncertainty,\u201d Journal of Risk and uncertainty ,\nvol. 5, pp. 297\u2013323, 1992.\n[17] \u201cExamining the international research evidence in relation to\nminimum passing distances for cyclists,\u201d Road Safety Authority\n(RSA) of Ireland, Tech. Rep., 2018, pre-legislative scrutiny report,\nRoad Safety Research and Driver Education. [Online]. Available:\nhttps://www.rsa.ie\n[18] M. Oskina, H. Farah, P. Morsink, R. Happee, and B. van Arem, \u201cSafety\nassessment of the interaction between an automated vehicle and a\ncyclist: a controlled field test,\u201d Transportation research record , vol.\n2677, no. 2, pp. 1138\u20131149, 2023.\n[19] Y .-C. Lee, \u201cMeasuring drivers\u2019 frustration in a driving simulator,\u201d in\nProceedings of the Human Factors and Ergonomics Society Annual\nMeeting , vol. 54, no. 19. Sage Publications Sage CA: Los Angeles,\nCA, 2010, pp. 1531\u20131535.\n[20] L. E. Suryana, S. Calvert, A. Zgonnikov, and B. van Arem, \u201cPrinciples\nand reasons behind automated vehicle decisions in ethically ambiguous\neveryday scenarios,\u201d arXiv preprint arXiv:2507.13837 , 2025.\n[21] S. Rahmani, J. Neumann, L. E. Suryana, C. Theunisse, S. C. Calvert,\nand B. Van Arem, \u201cA bi-level real-time microsimulation framework\nfor modeling two-dimensional vehicular maneuvers at intersections,\u201d\nin2023 IEEE 26th International Conference on Intelligent Transporta-\ntion Systems (ITSC) . IEEE, 2023, pp. 4221\u20134226.\n[22] S. Rahmani, S. C. Calvert, and B. van Arem, \u201cDecentralized modeling\nof vehicular maneuvers and interactions at urban junctions,\u201d Jul.\n2025. [Online]. Available: https://arxiv.org/abs/2507.21547\n[23] C. Hagemeister and L. Bertram, \u201cReported pushy driving against\ncyclists in germany,\u201d Journal of safety research , vol. 88, pp. 395\u2013405,\n2024.\n[24] A. G. Cunningham, E. Galceran, R. M. Eustice, and E. Olson, \u201cMpdm:\nMultipolicy decision-making in dynamic, uncertain environments for\nautonomous driving,\u201d in 2015 IEEE International Conference on\nRobotics and Automation (ICRA) . IEEE, 2015, pp. 1670\u20131677.\n[25] D. Mehta, G. Ferrer, and E. Olson, \u201cAutonomous navigation in\ndynamic social environments using multi-policy decision making,\u201d in\n2016 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS) . IEEE, 2016, pp. 1190\u20131197.",
  "project_dir": "artifacts/projects/Human_Reasons_Based_Supervision_Framework",
  "communication_dir": "artifacts/projects/Human_Reasons_Based_Supervision_Framework/.agent_comm",
  "assigned_at": "2025-08-03T21:03:08.110194",
  "status": "assigned"
}